{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated News Update, or, \"Dwyer's attempt at automating himself out of a job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* For the **requests** package: https://stackoverflow.com/questions/25067580/passing-web-data-into-beautiful-soup-empty-list\n",
    "* Stanford course on web scraping, notes: http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html\n",
    "* BeautifulSoup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* Selenium: https://stackoverflow.com/questions/13960326/how-can-i-parse-a-website-using-selenium-and-beautifulsoup-in-python\n",
    "* Digging through iframes: https://stackoverflow.com/questions/7534622/selecting-an-iframe-using-python-selenium https://stackoverflow.com/questions/23028664/python-beautifulsoup-iframe-document-html-extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "* 8/29/2018: Added Citylab, Electrek, cleaned code\n",
    "* 8/7/2018: Added Transport Reviews to academic paper scraper\n",
    "* 7/30/2018: Fixed GovTech scraper\n",
    "* 6/29/2018: Changed the whole scraper over to utilize a new class called *scraypah*. \n",
    "* 5/12/2018: Added Semiconductor Engineering scraper and academic articles scraper (~3 hours)\n",
    "* 4/13/2018: Integrated word document production through python\n",
    "* 3/19/2018: Added OEM/Gov section that quickly checks 17 sites for updates - only prints a notification that it needs to be checked if there are new updates from the past week\n",
    "* 2/27/2018: Wrote a function *page_scan* to more efficiently create the relevant web page dictionary \"profiles\"\n",
    "* 2/27/2018: Added 21CTP trucking news keywords to search for. Integrated functionality into existing web scraper.\n",
    "* 2/14/2018: Added NGV Global scraper for AFV stuff\n",
    "* 2/14/2018: Added fuel cells, hybrid, hybrid-electric, 'electric buses', 'electric truck', 'electric trucks', 'electric drive' to the search terms for AFVs...\n",
    "* 1/31/2018: Added *print_results* function to streamline printed results for each scraper. Added counter to track #articles that were too old. Added meta-data tracking capability (dumps into SQL database every week)\n",
    "* 1/31/2018: Split EV market analysis and web scraper into two different Notebooks\n",
    "* 1/26/2018: Added Lexology scraper\n",
    "* 1/19/2018: Fixed GreenCarCongress scraper (site redesign)\n",
    "* 1/4/2018: Added Engadget scraper\n",
    "* 1/4/2018: Added \"replace_em\" function to streamline removal of meaningless substrings from body text summaries\n",
    "* 12/29/2017: Added Reuters, MITNews, and ARSTechnica scrapers. Did some streamlining in the EV Sales analysis\n",
    "* 12/20/2017: Wrote up quick-guide to all the post-Python processing needed for the final News Update doc.\n",
    "* 12/20/2017: Changed to .xls format. Had to import a different package to do so, but makes mail merge work better\n",
    "* 12/13/2017: Fixed Trucks.com scraper - was pulling out the wrong date for each article (pulled a date from the sidebar...)\n",
    "* 12/8/2017: Edited Trucks.com search so that it doesn't pick up paragraph tags that are actually image captions (added condition that \"class = None\")\n",
    "* 12/8/2017: Added a bunch of comments, specifically in the first code segment (\"IEEE Spectrum\") for explanatory purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "* Add all relevant companies to keywords\n",
    "* Functionality to add news article to SQL database after the fact\n",
    "* Auto open word doc in gen_docx\n",
    "* MORE AUTOMATION\n",
    "* Fix Reuters, Quartz\n",
    "* Add Nikkei: https://asia.nikkei.com/\n",
    "* Add Phys.org\n",
    "* Add Jalopnik\n",
    "* Add some debugging assistance of some kind\n",
    "    * Quick \"request/bs4 of a url\"\n",
    "* Truck News\n",
    "* Transport Policy\n",
    "* [Journal of Modern Transportation](https://link.springer.com/journal/40534)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "html.parser - BeautifulSoup(markup, \"html.parser\")\n",
    "\n",
    "* Advantages: Batteries included, Decent speed, Lenient (as of Python 2.7.3 and 3.2.)\n",
    "\n",
    "* Disadvantages: Not very lenient (before Python 2.7.3 or 3.2.2)\n",
    "\n",
    "lxml - BeautifulSoup(markup, \"lxml\")\n",
    "\n",
    "* Advantages: Very fast, Lenient\n",
    "\n",
    "* Disadvantages: External C dependency\n",
    "\n",
    "html5lib - BeautifulSoup(markup, \"html5lib\")\n",
    "\n",
    "* Advantages: Extremely lenient, Parses pages the same way a web browser does, Creates valid HTML5\n",
    "\n",
    "* Disadvantages: Very slow, External Python dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:08.311107Z",
     "start_time": "2018-08-29T11:29:08.300133Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, define important stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:11.386721Z",
     "start_time": "2018-08-29T11:29:09.275978Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import datetime as dt\n",
    "import certifi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from docx.shared import Pt\n",
    "import time\n",
    "\n",
    "import win32com.client as win32\n",
    "import os\n",
    "\n",
    "tick_fontsize = 12\n",
    "axislabel_fontsize = 20\n",
    "title_fontsize = 25\n",
    "legend_fontsize = 15\n",
    "plt.rcParams.update({'legend.fontsize':15, 'xtick.labelsize': 15, 'ytick.labelsize': 15,\n",
    "                    'figure.titlesize': 30, 'axes.labelsize':25, 'xtick.major.pad': 8, 'ytick.major.pad':8, \n",
    "                    'axes.titlepad': 0, 'axes.labelpad': 15})\n",
    "sns.set(rc={\"figure.figsize\": (12, 9)}, style='ticks', context='poster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:11.407290Z",
     "start_time": "2018-08-29T11:29:11.403799Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install -c conda-forge --yes --prefix {sys.prefix} python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:11.511979Z",
     "start_time": "2018-08-29T11:29:11.426198Z"
    }
   },
   "outputs": [],
   "source": [
    "cav_keywords = ['self-driving','automated', 'self driving', 'autonomous', 'MaaS', 'ride-sharing', 'ridesharing', 'ride-hailing', \n",
    "                'ridehailing', 'lidar', 'LiDAR','rideshare', 'ridehail', 'ride-hail', 'ridesource', 'ride-source', 'ride-sourcing',\n",
    "                'carsharing', 'car-sharing', 'carshare', 'car-share', 'Uber', 'Lyft', 'Chariot', 'connected car']\n",
    "afv_keywords = ['rare-earth', 'rare earth', 'natural gas', 'electric vehicles', 'electric vehicle', 'electric car', 'EV', 'electrification', 'alternative fuel', 'CNG', 'LNG',\n",
    "                'alt-fuel', 'propane', 'charging stations', 'EVSE', 'electric vehicle charging', 'HEV', 'hybrid', 'hybrid-electric', 'plug-in', 'PHEV', 'electric motor',\n",
    "               'bio-fuel', 'biofuel', 'idle reduction', 'fuel cell', 'electric bus', 'electric buses', 'electric truck', 'electric trucks', 'electric drive',\n",
    "               'battery-electric', 'battery electric', 'battery-electric-powered']\n",
    "truck_keywords = ['alternative fuels', 'natural gas', 'compressed natural gas', 'liquefied natural gas', 'CNG', 'LNG', 'propane', 'LPG', 'dimethyl ether', 'DME', 'electric', 'electricity', 'electrified', 'electric drive', \n",
    "                  'battery', 'energy storage', 'hydrogen', 'fuel cell', 'hybrid', 'hybrid electric', 'hybrid hydraulic',' Phase 2', 'Phase II', 'efficiency', 'fuel efficiency', 'fuel economy', 'aftertreatment',\n",
    "                  'emission control', 'diesel particulate filter', 'DPF', 'selective catalytic reduction', 'SCR', 'aerodynamics', 'sustainability', 'waste heat recovery', 'Rankine', 'organic Rankine', 'SuperTruck', \n",
    "                  'automated manual', 'AMT', 'platooning', 'lithium', 'biofuel', 'fast charging', 'downspeed', 'downsize', 'clean diesel', 'turbocompound', 'rolling resistance', 'skirt', 'boat tail', 'axle', 'low viscosity',\n",
    "                  'catenary', 'autonomy', 'autonomous', 'connected and autonomous', 'connected', 'telematics', 'driver assist', 'CACC', 'active cruise control', 'crash avoidance', 'crashworthiness', 'weigh-in-motion', 'weigh in motion', \n",
    "                  'high productivity', 'truck size and weight', 'V2I', 'V2V', 'vehicle to infrastructure',' vehicle to vehicle',  'restructuring', 'acquisition', 'fuel cost', 'driver cost', 'operational efficiency', 'facility', \n",
    "                  'facilities', 'proving ground', 'partnership', 'regional haul', 'joint venture', 'grant', 'FOA', 'funding opportunity', 'unveil', 'announce', 'offer', 'expansion', 'greenhouse gas', 'GHG', 'emission regulation', \n",
    "                  'emissions regulation', 'idle', 'idling', 'zero emissions', 'strategic plan', 'SmartWay', 'VIUS', 'well to wheels', 'pump to wheels', 'well to pump', 'CARB', 'CEC', 'air resources board', 'energy commission', 'EPA', \n",
    "                  'Environmental Protection Agency', 'smart mobility', 'smart cities']\n",
    "hyperloop_keywords = ['hyperloop', 'high-speed train', 'high speed train', 'bullet train']\n",
    "\n",
    "# Used for diagnostics/tracking later\n",
    "scrape_specs = {}\n",
    "\n",
    "# Age filter, in days\n",
    "max_age = 7\n",
    "\n",
    "# For file naming and tracking\n",
    "search_date = str(dt.date.today())\n",
    "print('Time of search: {}'.format(search_date))\n",
    "\n",
    "# Needed for web scraping \"browser\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "# For database update; ensures duplicates aren't loaded\n",
    "db_update = False\n",
    "\n",
    "# driver = webdriver.Firefox(executable_path='geckodriver64.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:12.883380Z",
     "start_time": "2018-08-29T11:29:11.528923Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_count=0\n",
    "skip_count=0\n",
    "too_old=0\n",
    "iteration=0\n",
    "skip_ind = []\n",
    "old_ind = []\n",
    "def reset_trackers():\n",
    "    ''' Resets tracking metrics - executed before each scraper is run '''\n",
    "    global scraped_count\n",
    "    scraped_count=0\n",
    "    global skip_count\n",
    "    skip_count=0\n",
    "    global too_old\n",
    "    too_old=0\n",
    "    global iteration\n",
    "    iteration=0\n",
    "    global skip_ind\n",
    "    skip_ind = []\n",
    "    global old_ind\n",
    "    old_ind = []\n",
    "def replace_em(text):\n",
    "    '''Replaces odd characters in text. Used for page titles and summaries'''\n",
    "    bad_chars = ['â€œ', 'â€™', 'â€�','\\n', 'Â', 'â€”', '(earlier post)', 'â€?', '\\t', 'â€œ']\n",
    "    for bad_char in bad_chars:\n",
    "        text = text.replace(bad_char,'')\n",
    "    return text\n",
    "\n",
    "def grab_homepage(url):\n",
    "    '''Creates BeautifulSoup object using input url'''\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    page_1 = requests.get(url,headers=headers)\n",
    "    return BeautifulSoup(page_1.content, \"html5lib\")\n",
    "\n",
    "def print_results(site, scraped_count, skip_count, too_old, df, duration, scrape_specs):\n",
    "    '''Prints out a quick summary of one website's full scraping and adds summary specs to scrape_specs dictionary'''\n",
    "    print('{} {} article(s) scraped'.format(scraped_count,site)) \n",
    "    print('{} {} article(s) skipped due to error'.format(skip_count,site))  #(see urls_to_scrape[skip_ind+1])\n",
    "    print('{} {} article(s) skipped due to age'.format(too_old,site)) #(see urls_to_scrape[old_ind+1])\n",
    "    print('{} relevant article(s) collected'.format(df.shape[0])) \n",
    "    scrape_specs[f\"{site}\"] = {'Pages Scraped': scraped_count, 'Relevant Articles': df.shape[0], 'Errors': skip_count, \n",
    "                               'Too old': too_old, 'Time spent':duration}\n",
    "    return scrape_specs\n",
    "\n",
    "def page_scan(title, summary, url, date, source):\n",
    "    '''Searches a web page title and summary for keywords; returns the dictionary object that is used to create the final dataframe'''\n",
    "    afv_bool=0\n",
    "    cav_bool=0\n",
    "    truck_bool=0\n",
    "    hyperloop_bool=0\n",
    "    if any(keyword in title for keyword in hyperloop_keywords) | any(keyword in summary for keyword in hyperloop_keywords):\n",
    "        hyperloop_bool = 1\n",
    "    if any(keyword in title for keyword in cav_keywords) | any(keyword in summary for keyword in cav_keywords):\n",
    "        cav_bool = 1\n",
    "    if any(keyword in title for keyword in afv_keywords) | any(keyword in summary for keyword in afv_keywords):\n",
    "        afv_bool=1\n",
    "    if (any(keyword in title.lower() for keyword in truck_keywords)&(('truck' in title.lower())|('trucks' in title.lower()))) | \\\n",
    "       (any(keyword in summary.lower() for keyword in truck_keywords)&(('truck' in summary.lower())|('trucks' in summary.lower()))):\n",
    "        truck_bool=1\n",
    "    if (afv_bool == 1)|(cav_bool == 1)|(truck_bool == 1)|(hyperloop_bool == 1):\n",
    "        return {'title':title, 'summary':summary, 'link':url, 'source':source, 'date':date, 'AFV':afv_bool, 'CAV':cav_bool, \n",
    "                '21CTP':truck_bool, 'Hyperloop':hyperloop_bool}\n",
    "    else:\n",
    "        return 'Most definitely nope'\n",
    "\n",
    "### The following two functions are for the Word document output!\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    '''\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    '''\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "    \n",
    "    # bold the text\n",
    "    u = docx.oxml.shared.OxmlElement('w:b')\n",
    "#     u.set(docx.oxml.shared.qn('w:val'), 'single')\n",
    "    rPr.append(u)\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "CA_nums = 'NEED TO INSERT'\n",
    "def gen_docx(newstype, dwyer=True, CA_nums = CA_nums):\n",
    "    '''\n",
    "    Generates news Word doc using data file from web scrape\n",
    "    :param newstype: Either \"21CTP\", \"CAV\", or \"AFV\"\n",
    "    :param dwyer: If not running on Dwyer's computer, set this to False and put all needed files in the same directory\n",
    "    :param CA_nums: Input string for the CA EVSE numbers (automatically populates the caption for the EVSE bar chart figure)\n",
    "    '''\n",
    "    \n",
    "    # select data file (xls) based on the newstype and date. Note that search_date is a global variable defined outside\n",
    "    # of this function. Each news update only happens once a week --> only one xls file per newstype per week --> can't just\n",
    "    # pick any old search_date and make a file.\n",
    "    if dwyer:\n",
    "        data_file = f\"{newstype.lower()}_news_updates/{search_date}_{newstype}_news_download.xls\"    # Name of the excel file (standardized)\n",
    "    else:\n",
    "        data_file = f\"{search_date}_{newstype}_news_download.xls\"\n",
    "    \n",
    "    # Read the data in from the selected file\n",
    "    df = pd.read_excel(data_file)\n",
    "    df = df.reset_index(drop=True).T.to_dict()\n",
    "    \n",
    "    # Start creating the word doc\n",
    "    newsdoc = docx.Document(docx='python_docx.docx')\n",
    "    \n",
    "    # Add up-front stuff - title, headers, and for the AFV update, some other stuff (two captions and some text)\n",
    "    if newstype == 'AFV':\n",
    "        newsdoc.add_heading(f\"Alternative Fuel Vehicle Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\",0)\n",
    "        newsdoc.add_heading('EVSE Market Analysis', 1)\n",
    "        evse_bar_chart = newsdoc.add_paragraph().add_run('INSERT EVSE BAR CHART HERE')\n",
    "        evse_bar_chart.font.bold = True\n",
    "        evse_bar_chart.font.size=Pt(16)\n",
    "        evse_bar_chart.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "        newsdoc.add_paragraph('Figure: Number of EVSE plugs (note: not stations) by state and charging level.' \n",
    "                              'CA is not included, since it would make the rest of the state numbers illegible.' \n",
    "                              f\"CA holds a disproportionately large share of the total EVSE plugs: {CA_nums} \"\n",
    "                              'of Level 1, Level 2, and DCFC plugs respectively. Data Source: U.S. DOE AFDC Station Locator.',\n",
    "                             style='Caption')\n",
    "        newsdoc.add_paragraph(' ')\n",
    "        newsdoc.add_paragraph('The table below summarizes overall changes in number of EV charging stations by state between '\n",
    "                              f\"{(dt.date.today() - dt.timedelta(7)).strftime('%m/%d/%Y')} and {dt.date.today().strftime('%m/%d/%Y')}:\",\n",
    "                             style='Normal')\n",
    "        newsdoc.add_paragraph('Table 1: Change in number of EV charging stations by state, between '\n",
    "                              f\"{(dt.date.today() - dt.timedelta(7)).strftime('%m/%d/%Y')} and {dt.date.today().strftime('%m/%d/%Y')}\",\n",
    "                             style='Caption')\n",
    "        evse_bar_chart = newsdoc.add_paragraph().add_run('INSERT EVSE DELTA TABLE HERE')\n",
    "        evse_bar_chart.font.bold = True\n",
    "        evse_bar_chart.font.size=Pt(16)\n",
    "        evse_bar_chart.font.highlight_color = WD_COLOR_INDEX.YELLOW    \n",
    "    \n",
    "    if newstype == 'CAV':\n",
    "        newsdoc.add_heading(f\"Connected and Automated Vehicle Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\",0)\n",
    "        \n",
    "    if newstype == '21CTP':\n",
    "        newsdoc.add_heading(f\"21CTP Trucking Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\",0)\n",
    "    \n",
    "    for header in ['Business and Market Analysis','Technology, Testing, and Analysis','Policy and Government']:\n",
    "        newsdoc.add_heading(header, 1)\n",
    "        newsdoc.add_paragraph('')\n",
    "# Add all of the actual news items\n",
    "    for row in df:\n",
    "        row = df[row]\n",
    "        newsdoc.add_heading(row['title'],level=2)\n",
    "        p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "        p.add_run('(')\n",
    "        add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))   # This is where the add_hyperlink function is used\n",
    "        p.add_run(')')\n",
    "    if newstype == 'CAV':\n",
    "        newsdoc.add_heading('Relevant Transportation Research', 1)\n",
    "        newsdoc.add_paragraph('This section includes publications, papers, articles, and conferences that investigate and/or'\n",
    "                              'discuss transportation and travel demand impacts of MaaS or other “future travel” considerations.'\n",
    "                              'Portions of the abstract or description (not my words) are included under each title for more information.')\n",
    "    if dwyer:\n",
    "        newsdoc.save(f\"{newstype.lower()}_news_updates/Energetics {newstype} News Update - {search_date}.docx\")\n",
    "    else:\n",
    "        filename = f\"Energetics {newstype} News Update - {search_date}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "        \n",
    "def which_keyword_found(row):\n",
    "    '''\n",
    "    Identifies and stores which keywords triggered the news item pull\n",
    "    '''\n",
    "    words_found = []\n",
    "    for keyword in cav_keywords:\n",
    "        try:\n",
    "            if (row['summary'].find(keyword) > 0)|(row['title'].find(keyword) > 0):\n",
    "                words_found.append(keyword)\n",
    "        except:\n",
    "            continue\n",
    "    for keyword in afv_keywords:\n",
    "        try:\n",
    "            if (row['summary'].find(keyword) > 0)|(row['title'].find(keyword) > 0):\n",
    "                words_found.append(keyword)\n",
    "        except:\n",
    "            continue\n",
    "    return ', '.join(words_found)\n",
    "\n",
    "def keyword_pull(string):\n",
    "    '''\n",
    "    Pulls all capitalized words out of the title, as a quick \"keyword\" list\n",
    "    \n",
    "    https://stackoverflow.com/questions/13205343/code-to-detect-all-words-that-start-with-a-capital-letter-in-a-string\n",
    "    '''\n",
    "    try:\n",
    "        string = string.lstrip().split(' ')\n",
    "        keywords = []\n",
    "        for word in string:\n",
    "            try:\n",
    "                if (word[0].isupper()) & (word != 'The') & (word != 'This') & (word != 'I'):\n",
    "                    keywords.append(word)\n",
    "            except:\n",
    "                continue\n",
    "        return ', '.join(keywords)\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing class-based scraper\n",
    "Quartz doesn't work\n",
    "\n",
    "Bloomberg doesn't allow scraping\n",
    "\n",
    "Reuters doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:13.333150Z",
     "start_time": "2018-08-29T11:29:12.926191Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scraypah:\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.base_url = params['url']\n",
    "        self.source = params['source']\n",
    "        self.strainer = params['strain_bool']\n",
    "        if self.strainer:\n",
    "            self.strain_tag = params['strain_tag']\n",
    "            self.strain_attr_type = params['strain_attr_type']\n",
    "            self.strain_attr = params['strain_attr']\n",
    "        self.date_loc = params['date_loc']\n",
    "        self.date_format = params['date_format']\n",
    "        self.sum_loc = params['sum_loc']\n",
    "        self.title_loc = params['title_loc']\n",
    "        self.url_list_query = params['url_list_query']\n",
    "        \n",
    "    def get_urls(self):\n",
    "        self.urls_to_scrape=[]\n",
    "        if isinstance(self.base_url, str):\n",
    "            if not self.strainer:\n",
    "                page = requests.get(self.base_url, headers=headers)\n",
    "                time.sleep(0.5)\n",
    "                self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            else:\n",
    "                only_parse = SoupStrainer(self.strain_tag, attrs={self.strain_attr_type:self.strain_attr})\n",
    "                self.base_soup = BeautifulSoup(requests.get(self.base_url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "            self.urls_to_scrape = eval(self.url_list_query)\n",
    "        else:\n",
    "            for url in list(self.base_url):\n",
    "#                 print(url)\n",
    "                if not self.strainer:\n",
    "                    page = requests.get(url, headers=headers)\n",
    "                    time.sleep(1)\n",
    "                    self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                else:\n",
    "                    only_parse = SoupStrainer(self.strain_tag, attrs={self.strain_attr_type:self.strain_attr})\n",
    "                    self.base_soup = BeautifulSoup(requests.get(url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "#                 print('one base_soup')\n",
    "                self.urls_to_scrape += eval(self.url_list_query)\n",
    "#                 print(self.urls_to_scrape)\n",
    "        \n",
    "    def scrape_em(self):\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count=0\n",
    "        self.too_old=0\n",
    "        self.iteration=0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        for url in self.urls_to_scrape:\n",
    "            self.iteration+=1\n",
    "            try:\n",
    "                page = requests.get(url, headers=headers)\n",
    "                if self.source in ['Semiconductor Engineering', 'Reuters', 'Recode']:\n",
    "                    article=BeautifulSoup(page.content, \"html5lib\")\n",
    "                else:\n",
    "                    article=BeautifulSoup(page.content, \"lxml\")\n",
    "                date = pd.to_datetime(eval(self.date_loc).strip().replace('\\\\xa0','').replace(' -\\nBy:',''), format=self.date_format).date()\n",
    "                if (date - dt.date.today()).days >= -max_age:\n",
    "                    if self.source == 'Autoblog':\n",
    "                        try:\n",
    "                            summary = eval(self.sum_loc)\n",
    "                            summary = replace_em(summary[0].text+ ' '+summary[1].text+ ' '+summary[2].text)\n",
    "                        except:\n",
    "                            summary = ' '.join(article.find('div', attrs={'class':'post-body'}).text.replace('\\\\t','').replace('\\\\n\\\\n', '\\n').split('\\n')[1:4])\n",
    "                    else:\n",
    "                        summary = eval(self.sum_loc)\n",
    "                        summary = replace_em(summary[0].text+ ' '+summary[1].text+ ' '+summary[2].text)\n",
    "                    title = eval(self.title_loc).replace('â€™',\"'\").replace('\\\\xa0',' ').replace('\\\\n','').lstrip().replace('  ','')\n",
    "                    temp = page_scan(title, summary, url, date, self.source)\n",
    "                    if temp != 'Most definitely nope':\n",
    "                        self.relevant_articles[self.scraped_count] = temp\n",
    "                    self.scraped_count+=1\n",
    "                else:\n",
    "                    self.too_old += 1\n",
    "                    self.old_ind.append(self.iteration-1)\n",
    "            except Exception as exc:\n",
    "                print(f\"{str(exc)}: {url}\")\n",
    "                self.skip_count+=1\n",
    "                self.skip_ind.append(self.iteration-1)\n",
    "                continue\n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:13.788033Z",
     "start_time": "2018-08-29T11:29:13.352048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraper_dict = {'MIT': {'url':'http://news.mit.edu/mit-news', \n",
    "                        'source': 'MIT',\n",
    "                        'strain_tag':'ul', \n",
    "                        'strain_attr_type':'class', \n",
    "                        'strain_attr':'view-mit-news clearfix', \n",
    "                        'url_list_query':\"['http://news.mit.edu'+item.a['href'] for item in self.base_soup.find('ul', class_='view-mit-news clearfix').find_all('li')]\",\n",
    "                        'date_loc': \"article.find('span', attrs={'itemprop':'datePublished'}).text\", \n",
    "                        'date_format':None,\n",
    "                        'sum_loc': \"article.find('div', attrs={'class': 'field-item even'}).find_all('p')\",\n",
    "                        'title_loc':\"article.find('h1', attrs={'class':'article-heading'}).text\", \n",
    "                        'strain_bool':True},\n",
    "                'SemEng': {'url':'http://semiengineering.com/category-main-page-iot-security/', \n",
    "                          'source': 'Semiconductor Engineering', \n",
    "                          'strain_tag':'div', \n",
    "                          'strain_attr_type':'class', \n",
    "                          'strain_attr':'l_col', \n",
    "                          'url_list_query':\"[item['href'] for item in self.base_soup.find('div', class_='l_col').find_all('a', href=True,title=True)]\",\n",
    "                          'date_loc': \"article.find('div',class_='loop_post_meta').contents[0]\", \n",
    "                          'date_format':None,\n",
    "                          'sum_loc': \"article.find('div', class_='post_cnt post_cnt_first_letter').find_all('p')[1:4]\",\n",
    "                          'title_loc':\"article.find('h1', class_='post_title').text\", \n",
    "                          'strain_bool':True},\n",
    "                'Quartz': {'url':'https://qz.com/search/self-driving', \n",
    "                           'source': 'Quartz', \n",
    "                           'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all(class_='queue-article')]\",\n",
    "                           'date_loc': \"article.find('span', attrs={'class':'timestamp'}).text\", \n",
    "                           'date_format':None,\n",
    "                           'sum_loc': \"article.find_all('p')[:3]\",\n",
    "                           'title_loc':\"article.find('h1').text\", \n",
    "                           'strain_bool':False},\n",
    "                'Recode': {'url':'https://www.recode.net/', \n",
    "                           'source': 'Recode', \n",
    "                           'strain_tag':'a', \n",
    "                           'strain_attr_type':'data-analytics-link', \n",
    "                           'strain_attr':'article', \n",
    "                           'url_list_query':\"[item['href'] for item in self.base_soup.find_all('a', attrs={'data-analytics-link':'article'})]\",\n",
    "                           'date_loc': \"article.time.text.replace('\\\\n', '')\", \n",
    "                           'date_format':None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc':\"article.h1.text\", \n",
    "                           'strain_bool':True},\n",
    "                'GovTech': {'url':'http://www.govtech.com/fs/transportation/',\n",
    "                            'source':'GovTech', \n",
    "                            'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all(class_=['sub-feature-article','feature-article'])]\",\n",
    "                            'date_loc':\"article.find('span', class_='date').text.strip()\", \n",
    "                            'date_format':None,\n",
    "                            'sum_loc':\"[item for item in article.find(class_='col-md-10').find_all('div') if len(str(item)) > 12] \\\n",
    "                                        if len([item for item in article.find(class_='col-md-10').find_all('p')]) < 3 \\\n",
    "                                        else [item for item in article.find(class_='col-md-10').find_all('p')]\" ,\n",
    "                            'title_loc': \"article.find('h1').text.strip()\", \n",
    "                            'strain_bool':False},\n",
    "                'Reuters': {'url':'https://www.reuters.com/news/technology', \n",
    "                            'source': 'Reuters', \n",
    "                            'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h2', class_='headline_ZR_Fh')]\",\n",
    "                            'date_loc': \"article.find('div', attrs={'class':'date_V9eGk'}).text.split('/')[0]\", \n",
    "                            'date_format':None,\n",
    "                            'sum_loc': \"article.find('div', attrs={'class':'body_1gnLA'}).find_all('p')\",\n",
    "                            'title_loc':\"article.h1.text\", \n",
    "                            'strain_bool':False},\n",
    "                'CityLab': {'url':'https://www.citylab.com/transportation/', \n",
    "                            'source': 'Citylab', \n",
    "                            'strain_tag':['h2','h1'], \n",
    "                            'strain_attr_type':'class', 'strain_attr':['c-promo__hed','c-river-item__hed c-river-item__hed--'], \n",
    "                            'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all(['h1','h2'], class_=['c-promo__hed','c-river-item__hed c-river-item__hed--'])]\",\n",
    "                            'date_loc': \"article.time.text\", \n",
    "                            'date_format':None,\n",
    "                            'sum_loc': \"article.find_all('p')[1:]\",\n",
    "                            'title_loc':\"article.h1.text\", \n",
    "                            'strain_bool':True},\n",
    "                'Engadget': {'url':['https://www.engadget.com/tags/transportation/','https://www.engadget.com/tag/transportation/page/2/'], \n",
    "                             'source': 'Engadget', \n",
    "                             'strain_tag':'a', \n",
    "                             'strain_attr_type':'class', \n",
    "                             'strain_attr':'o-hit__link', \n",
    "                             'url_list_query':\"['https://www.engadget.com'+item['href'] for item in self.base_soup.find_all('a', attrs={'class':'o-hit__link'})]\",\n",
    "                             'date_loc': \"article.find('meta', attrs={'name':'published_at'})['content']\", \n",
    "                             'date_format':None,\n",
    "                             'sum_loc': \"article.find('div', attrs={'class':'container@m-'}).find_all('p')\",\n",
    "                             'title_loc':\"article.title.text\", \n",
    "                             'strain_bool':True},\n",
    "                'Autoblog': {'url':'https://www.autoblog.com/',\n",
    "                             'source': 'Autoblog',\n",
    "                             'strain_tag':'li',\n",
    "                             'strain_attr_type':'class', \n",
    "                             'strain_attr':'flex-item promo-list-item',\n",
    "                             'url_list_query':\"['https://www.autoblog.com/'+item.a['href'] for item in self.base_soup.find_all('li', attrs={'class':'flex-item promo-list-item'})]\",\n",
    "                             'date_loc': \"article.find('div', attrs={'class':'post-date'}).text\", \n",
    "                             'date_format':None,\n",
    "                             'sum_loc': \"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\n",
    "                             'title_loc':\"article.h1.text\", \n",
    "                             'strain_bool':True},\n",
    "                'Electrek': {'url':'https://electrek.co/', \n",
    "                             'source': 'Electrek', \n",
    "                             'strain_tag':'h1', \n",
    "                             'strain_attr_type':'class', 'strain_attr':'post-title', \n",
    "                             'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h1', class_='post-title')]\",\n",
    "                             'date_loc': \"article.find('p', class_='time-twitter').text\", \n",
    "                             'date_format':None,\n",
    "                             'sum_loc': \"article.find('div', class_='post-body').find_all('p')[1:]\",\n",
    "                             'title_loc':\"article.find('h1', class_='post-title').text\", \n",
    "                             'strain_bool':True},\n",
    "                'The Verge': {'url':'https://www.theverge.com/transportation',\n",
    "                              'source': 'The Verge',\n",
    "                              'strain_tag':'h2', \n",
    "                              'strain_attr_type':'class', 'strain_attr':'c-entry-box--compact__title', \n",
    "                              'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h2', class_='c-entry-box--compact__title')]\",\n",
    "                              'date_loc': \"article.time.text\",\n",
    "                              'date_format':None,\n",
    "                              'sum_loc': \"article.find_all('p')\",\n",
    "                              'title_loc':\"article.h1.text\",\n",
    "                              'strain_bool':True},\n",
    "                'TechCrunch': {'url':['https://techcrunch.com/', 'https://techcrunch.com/page/2/', 'https://techcrunch.com/page/3/','https://techcrunch.com/page/4/'], 'source': 'TechCrunch',\n",
    "                               'strain_tag':'a', \n",
    "                               'strain_attr_type':'class', \n",
    "                               'strain_attr':'post-block__title__link',\n",
    "                               'url_list_query':\"[item['href'] for item in self.base_soup.find_all('a', class_='post-block__title__link')]\",\n",
    "                               'date_loc': \"url[23:33]\", \n",
    "                               'date_format':None,\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'article-content'}).find_all('p')\",\n",
    "                               'title_loc':\"article.find('h1', attrs={'class':'article__title'}).text\", \n",
    "                               'strain_bool':True},\n",
    "                'NGV Global': {'url':'http://www.ngvglobal.com/', \n",
    "                               'source': 'NGV Global', \n",
    "                               'strain_tag':'h2', \n",
    "                               'strain_attr_type':'class', \n",
    "                               'strain_attr':'entry-title', \n",
    "                               'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h2', attrs={'class':'entry-title'})]\",\n",
    "                               'date_loc': \"article.find('time')['title']\", \n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'pf-content'}).find_all('p')\",\n",
    "                               'title_loc':\"article.find('h1', attrs={'class':'entry-title'}).text\", \n",
    "                               'strain_bool':True},\n",
    "                'Charged EVs': {'url':['https://chargedevs.com/category/newswire/','https://chargedevs.com/category/newswire/page/2/'],\n",
    "                                'source':'Charged EVs', \n",
    "                                'strain_tag':'h3',\n",
    "                                'strain_attr_type':'class',\n",
    "                                'strain_attr':'h2', \n",
    "                                'url_list_query':'[item.a[\"href\"] for item in self.base_soup.find_all(\"h3\", class_=\"h2\")]',\n",
    "                                'date_loc':\"article.find('time').text\", \n",
    "                                'date_format':None,\n",
    "                                'sum_loc':\"article.find('section',class_='entry-content clearfix').find_all('p')\",\n",
    "                                'title_loc': \"article.find('h2', class_='page-title').text\", \n",
    "                                'strain_bool':True},\n",
    "               'ARS Technica': {'url':'https://arstechnica.com/cars/', \n",
    "                                'source': 'ARS Technica', \n",
    "                                'strain_tag':'a', \n",
    "                                'strain_attr_type':'class', \n",
    "                                'strain_attr':'overlay', \n",
    "                                'url_list_query':\"[item['href'] for item in self.base_soup.find_all('a', attrs={'class': 'overlay'})]\",\n",
    "                                'date_loc': \"article.find('time', attrs={'class':'date'}).text\", \n",
    "                                'date_format':None,\n",
    "                                'sum_loc': \"article.find('div', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\",\n",
    "                                'title_loc':\"article.h1.text\", \n",
    "                                'strain_bool':True},\n",
    "                'IEEE Spectrum': {'url':'https://spectrum.ieee.org/transportation', 'source': 'IEEE Spectrum', \n",
    "                                  'strain_tag':'article',\n",
    "                                  'strain_attr_type':'class', \n",
    "                                  'url_list_query': \"['https://spectrum.ieee.org'+item.a['href'] for item in self.base_soup.find_all('article')]\",\n",
    "                                  'strain_attr':'item sml_article transportation',\n",
    "                                  'date_loc': \"article.label.text\", \n",
    "                                  'date_format':'%d %b %Y | %H:%M GMT',\n",
    "                                  'sum_loc': \"article.find_all('p', limit=5)\",\n",
    "                                  'title_loc':\"article.h1.text\", \n",
    "                                  'strain_bool':True},\n",
    "                'GreenCarCongress': {'url':['http://www.greencarcongress.com/', 'http://www.greencarcongress.com/page/2/'], \n",
    "                                     'source': 'GreenCarCongress', \n",
    "                                     'strain_tag':'article', \n",
    "                                     'strain_attr_type':'class', \n",
    "                                     'strain_attr':'post entry', \n",
    "                                     'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\", \n",
    "                                     'date_format':None,\n",
    "                                     'sum_loc': \"article.find_all('p', limit=5)\",\n",
    "                                     'title_loc':\"article.h2.a.text\", \n",
    "                                     'strain_bool':True},\n",
    "#                 'Bloomberg': {'url':['https://www.bloomberg.com/search?query=self+driving','https://www.bloomberg.com/search?query=electric%20vehicles'], 'source': 'Bloomberg', \n",
    "#                               'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h1')]\",\n",
    "#                               'date_loc': \"article.find('time', attrs={'class':'article-timestamp'})['datetime']\", 'date_format':None,\n",
    "#                               'sum_loc': \"article.find('div', attrs={'class':'body-copy-v2 fence-body'}).find_all('p')\",\n",
    "#                               'title_loc':\"article.find('h1', attrs={'class':'lede-text-v2__hed'}).text\", 'strain_bool':False},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:36:54.663884Z",
     "start_time": "2018-08-29T11:29:13.805834Z"
    }
   },
   "outputs": [],
   "source": [
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for site in list(scraper_dict.keys()):\n",
    "    temp_start_time = time.time()\n",
    "    print('\\n'+site.upper())\n",
    "    scraypahs[site] = scraypah(scraper_dict[site])\n",
    "    scraypahs[site].get_urls()\n",
    "    scraypahs[site].scrape_em()\n",
    "    scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count, \n",
    "                             scraypahs[site].too_old, scraypahs[site].relevant_df, round(time.time()-temp_start_time,2), \n",
    "                             scrape_specs)\n",
    "\n",
    "# Trucks.com scraper is unique, can't use standard class\n",
    "start_time=time.time()\n",
    "url = 'https://www.trucks.com/'\n",
    "soup = BeautifulSoup(requests.get(url,headers=headers).content, \"html5lib\")\n",
    "trucks_urls_to_scrape = []\n",
    "\n",
    "for item in soup.find_all('div', attrs={'class':'content-block'}):\n",
    "    try:\n",
    "        trucks_urls_to_scrape.append({'link':item.find('div', attrs={'class':'title'}).a['href'],\n",
    "                                      'date':pd.to_datetime(item.find('div', attrs={'class':'date'}).text + ', {}'.format(dt.date.today().year))})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('\\nTRUCKS.COM')\n",
    "trucks_relevant_articles = {}\n",
    "reset_trackers()\n",
    "source = 'Trucks.com'\n",
    "\n",
    "for url in trucks_urls_to_scrape:\n",
    "    iteration+=1\n",
    "    date = url['date'].date()\n",
    "    \n",
    "    try:\n",
    "        if (date - dt.date.today()).days >= -max_age:\n",
    "            page = requests.get(url['link'], headers=headers)\n",
    "            article=BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "            summary = article.find('section', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\n",
    "            summary = replace_em(summary[0].text+ ' '+summary[1].text+ ' '+summary[2].text)\n",
    "            title = article.h1.text.replace('â€™',\"'\").replace('\\xa0',' ').replace('\\n','').lstrip().replace('  ','')\n",
    "\n",
    "            temp = page_scan(title, summary, url['link'], date, source)\n",
    "            if temp != 'Most definitely nope':\n",
    "                trucks_relevant_articles[scraped_count] = temp\n",
    "            scraped_count+=1\n",
    "        \n",
    "        else:\n",
    "            too_old += 1\n",
    "            old_ind.append(iteration-1)\n",
    "        \n",
    "    except:\n",
    "        skip_count+=1\n",
    "        skip_ind.append(iteration-1)\n",
    "        continue\n",
    "        \n",
    "trucks_df = pd.DataFrame.from_dict(trucks_relevant_articles).T\n",
    "scrape_specs = print_results(source, scraped_count, skip_count, too_old, trucks_df, round(time.time()-start_time,2), scrape_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:53:09.986756Z",
     "start_time": "2018-08-29T10:53:09.726233Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.citylab.com/transportation/'\n",
    "page = requests.get(citylab.urls_to_scrape[0], headers = headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:04:21.403090Z",
     "start_time": "2018-08-29T11:04:21.395162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('p')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:27:36.447897Z",
     "start_time": "2018-08-29T11:27:36.439918Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict = {'url':'https://electrek.co/', \n",
    "             'source': 'Electrek', \n",
    "             'strain_tag':'h1', \n",
    "             'strain_attr_type':'class', 'strain_attr':'post-title', \n",
    "             'url_list_query':\"[item.a['href'] for item in self.base_soup.find_all('h1', class_='post-title')]\",\n",
    "             'date_loc': \"article.find('p', class_='time-twitter').text\", \n",
    "             'date_format':None,\n",
    "             'sum_loc': \"article.find('div', class_='post-body').find_all('p')[1:]\",\n",
    "             'title_loc':\"article.find('h1', class_='post-title').text\", \n",
    "             'strain_bool':True\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:27:36.799851Z",
     "start_time": "2018-08-29T11:27:36.794866Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_a_scraypah(attr_dict):\n",
    "    scraper = scraypah(attr_dict)\n",
    "    scraper.get_urls()\n",
    "    scraper.scrape_em()\n",
    "    return scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:27:39.852150Z",
     "start_time": "2018-08-29T11:27:37.262011Z"
    }
   },
   "outputs": [],
   "source": [
    "electrek = test_a_scraypah(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:27:41.683042Z",
     "start_time": "2018-08-29T11:27:41.661433Z"
    }
   },
   "outputs": [],
   "source": [
    "electrek.relevant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:36:54.962081Z",
     "start_time": "2018-08-29T11:36:54.777545Z"
    }
   },
   "outputs": [],
   "source": [
    "scrape_specs_df = pd.DataFrame.from_dict(scrape_specs).T.reset_index()\n",
    "scrape_specs_df['Time per relevant article'] = scrape_specs_df['Time spent']/scrape_specs_df['Relevant Articles']\n",
    "display(scrape_specs_df)\n",
    "\n",
    "all_news_dfs = []\n",
    "for key, value in scraypahs.items():\n",
    "    all_news_dfs.append(value.relevant_df)\n",
    "\n",
    "### Stack all of the articles into a single dataframe and do some cleaning (drop duplicate articles)\n",
    "all_df = pd.concat(all_news_dfs)\n",
    "all_df = all_df[['title', 'date', 'AFV', 'CAV', '21CTP', 'Hyperloop', 'summary', 'source', 'link']].sort_values('date', ascending=False)\n",
    "all_df.drop_duplicates(subset='title', inplace=True)\n",
    "all_df = all_df.replace('\\$','$', regex=True)\n",
    "\n",
    "print('Smart Mobility articles found: {}'.format(all_df['CAV'].sum().astype(int)))\n",
    "print('Alternative Fuel Vehicle articles found: {}'.format(all_df['AFV'].sum().astype(int)))\n",
    "print('21CTP articles found: {}'.format(all_df['21CTP'].sum().astype(int)))\n",
    "print('Hyperloop articles found: {}'.format(all_df['Hyperloop'].sum().astype(int)))\n",
    "\n",
    "### Populate meta-data columns\n",
    "all_df['reason_for_tag'] = all_df.apply(which_keyword_found, axis=1)\n",
    "all_df['keywords'] = all_df['title'].apply(keyword_pull)\n",
    "\n",
    "### Format for excel writing\n",
    "AFV_news = all_df[all_df['AFV'] == 1].sort_values('date', ascending=False)\n",
    "CAV_news = all_df[all_df['CAV'] == 1].sort_values('date', ascending=False)\n",
    "truck_news = all_df[all_df['21CTP'] == 1].sort_values('date', ascending=False)\n",
    "hyperloop_news = all_df[all_df['Hyperloop'] == 1].sort_values('date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:42:25.878600Z",
     "start_time": "2018-08-29T11:42:25.684744Z"
    }
   },
   "outputs": [],
   "source": [
    "if (dt.date.today().weekday() == 0):\n",
    "    print('Monday!')\n",
    "    filename = f'cav_news_updates/{search_date}_cav_news_download.xls'\n",
    "    CAV_news.to_excel(filename)\n",
    "    if hyperloop_news.shape[0] > 0:\n",
    "        filename2 = f'hyperloop_news_updates/{search_date}_hyperloop_news_download.xls'\n",
    "        hyperloop_news.to_excel(filename2)\n",
    "        print('Some hyperloop stuff!')\n",
    "elif (dt.date.today().weekday() == 2):\n",
    "    print('Wednesday!')\n",
    "    filename = f'afv_news_updates/{search_date}_afv_news_download.xls'\n",
    "    AFV_news.to_excel(filename)\n",
    "elif (dt.date.today().weekday() == 4):\n",
    "    print('Friday!')\n",
    "    filename = f'21CTP_news_updates/{search_date}_21CTP_news_download.xls'\n",
    "    truck_news.to_excel(filename)\n",
    "\n",
    "# # Open excel file to edit or add any additional news items\n",
    "# cwd = os.getcwd()\n",
    "# xls_file = cwd+'/'+filename\n",
    "\n",
    "# excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "# excel.Visible = True\n",
    "\n",
    "# # open the file\n",
    "# excel.Workbooks.Open(xls_file)\n",
    "\n",
    "# # wait before closing\n",
    "# _ = input(\"Press enter to close Excel: \")\n",
    "# excel.Application.Quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:18.825518Z",
     "start_time": "2018-08-27T11:25:25.058305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open excel file to edit or add any additional news items\n",
    "cwd = os.getcwd()\n",
    "xls_file = cwd+'/'+filename\n",
    "\n",
    "excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "excel.Visible = True\n",
    "\n",
    "# open the file\n",
    "excel.Workbooks.Open(xls_file)\n",
    "\n",
    "# wait before closing\n",
    "_ = input(\"Press enter to close Excel: \")\n",
    "excel.Application.Quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate xls and database to track news items\n",
    "Only run with **final** news item spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:22.380653Z",
     "start_time": "2018-08-27T11:58:22.374653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_week = str((pd.to_datetime(search_date) - dt.timedelta(days=7)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:23.680237Z",
     "start_time": "2018-08-27T11:58:23.423569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('news_updates.db')\n",
    "if (dt.date.today().weekday() == 0)&(~db_update):\n",
    "    print('CAV')\n",
    "    pd.read_excel('cav_news_updates/{}_cav_news_download.xls'.format(search_date)).drop('21CTP', axis=1).to_sql('CAV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "elif (dt.date.today().weekday() == 2)&(~db_update):\n",
    "    print('AFV')\n",
    "    pd.read_excel('afv_news_updates/{}_afv_news_download.xls'.format(search_date)).drop('21CTP', axis=1).to_sql('AFV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "conn.close()\n",
    "\n",
    "if dt.date.today().weekday() == 2:\n",
    "    print('Uploaded metadata! So many datas!')\n",
    "    conn = sqlite3.connect('news_updates_meta.db')\n",
    "    scrape_specs_df.drop(['Time spent', 'Time per relevant article'], axis=1).to_sql('news_updates_meta', conn, if_exists='append', index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python docx\n",
    "https://github.com/python-openxml/python-docx/issues/384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:25.815349Z",
     "start_time": "2018-08-27T11:58:25.683697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dt.date.today().weekday() == 2:\n",
    "    print('AFV')\n",
    "    gen_docx('AFV')\n",
    "elif dt.date.today().weekday() == 0:\n",
    "    print('CAV')\n",
    "    gen_docx('CAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:22:01.531867Z",
     "start_time": "2018-07-23T16:22:01.498921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_docx('21CTP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic articles\n",
    "Still preliminary . . . needs more honing/tinkering. Should also add other sites.\n",
    "\n",
    "NEED TO ADD: TRB!!!\n",
    "\n",
    "https://www.springer.com/engineering/civil+engineering/journal/42421?TrucksFoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:35.089220Z",
     "start_time": "2018-08-27T11:58:28.623754Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path='geckodriver64.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:58:35.428102Z",
     "start_time": "2018-08-27T11:58:35.336556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paypuh_scraypuh(url, source):\n",
    "    '''\n",
    "    bad_egg: Missing a key component (usually abstract), so skip printout/tracking\n",
    "    still_more: Date is still within past week, continue scraping!\n",
    "    '''\n",
    "    soup = grab_homepage(url)\n",
    "    papers_to_scrape = [paper.a['href'] for paper in soup.find_all('div', attrs={'class':'pod-listing-header'})]\n",
    "    still_more = True\n",
    "    scraped_count=0\n",
    "    papers = {}\n",
    "\n",
    "    for paper in papers_to_scrape:\n",
    "        bad_egg=False\n",
    "        if not still_more:\n",
    "            break\n",
    "        driver.get(paper)\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\"span[class='CollapseText']\").click()\n",
    "            date = pd.to_datetime(driver.find_element_by_css_selector(\"dl[class='articleDates smh']\").text.split('Available online ')[1]).date()\n",
    "            soup = BeautifulSoup(driver.page_source,\"html5lib\")\n",
    "            if (date - dt.date.today()).days > -max_age:\n",
    "                title = soup.find('h1',class_='svTitle').text\n",
    "                summary = soup.find('div',class_='abstract svAbstract ').p.text\n",
    "            else:\n",
    "                still_more=False\n",
    "        except:\n",
    "            try:\n",
    "                driver.find_element_by_css_selector(\"button[class='show-hide-details']\").click()\n",
    "                soup = BeautifulSoup(driver.page_source,\"html5lib\")\n",
    "                date = pd.to_datetime(soup.find('div',class_='wrapper').p.text.split('Available online ')[1]).date()\n",
    "                if (date - dt.date.today()).days > -max_age:\n",
    "                    title = soup.find('span',class_='title-text').text\n",
    "                    summary = soup.find('div',class_='abstract author').p.text\n",
    "                else:\n",
    "                    still_more=False\n",
    "            except:\n",
    "                bad_egg = True\n",
    "                print('bad egg in {}: {}'.format(source, paper))\n",
    "                pass\n",
    "        scraped_count+=1\n",
    "\n",
    "        if still_more and not bad_egg:\n",
    "            papers[scraped_count] = {'title':title, 'summary':summary, 'link':paper, 'source':source, 'date':date}\n",
    "            \n",
    "    print('{} new papers in {}'.format(scraped_count,source))\n",
    "\n",
    "    return pd.DataFrame(papers).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:59:37.240576Z",
     "start_time": "2018-08-27T11:58:35.610571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tparta = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles', 'Transportation Part A')\n",
    "tpartb = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles','Transportation Part B')\n",
    "tpartc = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles','Transportation Part C')\n",
    "tpartd = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles','Transportation Part D')\n",
    "tparte = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles','Transportation Part E')\n",
    "tpartf = paypuh_scraypuh('https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles','Transportation Part F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:59:51.602235Z",
     "start_time": "2018-08-27T11:59:37.533748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'Transport Reviews'\n",
    "soup = BeautifulSoup(requests.get('https://www.tandfonline.com/action/showAxaArticles?journalCode=ttrv20').content, 'lxml')\n",
    "date = pd.to_datetime(soup.find(class_='tocEPubDate').text.split(':')[1])\n",
    "url = 'https://www.tandfonline.com' + soup.find(class_='tocEPubDate').find_parent().find_parent().find_all('a')[0]['href']\n",
    "\n",
    "transport_review_dict = {}\n",
    "i=0\n",
    "for article in soup.find_all('div', class_=\"tocArticleEntry include-metrics-panel\"):\n",
    "    i += 1\n",
    "    date = pd.to_datetime(article.find(class_='tocEPubDate').text.split(':')[1]).date()\n",
    "    if (dt.date.today() - date).days < 10:\n",
    "        url = 'https://www.tandfonline.com' + article.find(class_='tocEPubDate').find_parent().find_parent().find_all('a')[0]['href']\n",
    "        article_soup = BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "        title = article_soup.find('span', class_='NLM_article-title hlFld-title').text\n",
    "        abstract = article_soup.find('div', class_='abstractSection abstractInFull').text\n",
    "        transport_review_dict[i] = {'title':title, 'summary':abstract, 'link':url, 'source':source, 'date':date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:59:51.896399Z",
     "start_time": "2018-08-27T11:59:51.886426Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week_o_papers = pd.concat([tparta, tpartb, tpartc, tpartd, tparte, tpartf, pd.DataFrame.from_dict(transport_review_dict)])\n",
    "# week_o_papers.to_excel('{} papers.xls'.format(search_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:59:52.307094Z",
     "start_time": "2018-08-27T11:59:52.206333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsdoc = docx.Document(docx='python_docx.docx')\n",
    "\n",
    "for row in week_o_papers.reset_index(drop=True).T:\n",
    "    row = week_o_papers.iloc[row,:]\n",
    "    newsdoc.add_heading(row['title'],level=2)\n",
    "    p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "    p.add_run('(')\n",
    "    add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))\n",
    "    p.add_run(')')\n",
    "newsdoc.save('{} papers.docx'.format(search_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T11:59:52.652299Z",
     "start_time": "2018-08-27T11:59:52.603269Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('news_papers.db')\n",
    "week_o_papers.to_sql('news_papers', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498px",
    "left": "1480.3px",
    "right": "20px",
    "top": "119.976px",
    "width": "658px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
